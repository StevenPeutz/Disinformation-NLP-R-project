---
title: "Pro-Kremlin Misinformation"
output:
  html_document:
    df_print: paged
    code_folding: hide
---
 
## Introduction

Disinformation, defined as the subset of 'misinformation' where there is intent to mislead, has seen a an astronomical rise in both its success in terms of spread and impact as well in the effort to combat it. Though known state-backed disinformation campaigns date back to at least the cold war era, they have perhaps caught the eye of the public after the 2016 US presidential election (Allcot et al., 2017). Later in 2016 disinformation is claimed to have been influential in the 'Brexit' referendum, and later in 2018 a similar development was suspected during the Brazilian presidential elections. As this was unfolding an explosive growth in so called 'fact-checking' organisation and their cooperation with news agencies, social media companies (e.g. facebook/meta) and governments can be seen. Fact-checking organization, often volunteer based or financed through charity, their capacity tend to be outpaced by the sheer volume of suspected disinformation content.

One supposed solution for this is using AI to automate classification of new articles or posts based on its linguistic aspects. Though improvements are being made here, the most accurate models are strongly dependent on meta data such as publication network which are often not available when shared through social media, and have other downsides such as disproportionately high false positive rates when publication networks that have shared disinformation were to post content without disinformation.

An alternative 'intermediate' solution is offered in this project. Instead of classification we aim to discover the topics that are present in Russian disinformation. By doing this we can streamline fact-checking by establishing a basis on which already fact-checked disinformation can be matched with with newly published unchecked articles and posts. We use Latent Dirichlet topic modelling (LDA) in order to create our mixed membership model. We use LDA because it is an algorithm that uses a three level hierarchical Bayesian model in which each item of a collection is modeled as a finite mixture over an underlying set of topics (Blei et al., 2001).



## Project scope

This projects restricts itself to pro-Kremlin disinformation. This is specified as 'pro-Kremlin' as direct ties with the Russian Internet Research Agency (IRA) and official backing of the Kremlin are perhaps expected but are not verified with hard proof.
The scoped will also be limited to a preliminary test of the theoretical possibility and validity of the LDA mixed membership modelling without going into the practical application of the results.



## About the data:

The disinformation texts were collected by the EUvsDisinfo project. A project started in 2015 that identifies and fact checks disinformation cases originating from pro-Kremlin media that are spread across the EU. 
More information about this project can be found here: https://euvsdisinfo.eu/
The dataset can be found here: https://www.kaggle.com/datasets/stevenpeutz/misinformation-fake-news-text-dataset-79k



## References

Allcott, H., Gentzkow, M. Social media and fake news in the 2016 election. Journal of Economic Perspectives 31, 2 (2017), 211–36.  

Arun R., Suresh V., Madhavan V., Murthy N. On finding the natural number of topics with latent dirichlet allocation: Some observations. In Advances in knowledge discovery and data mining, Mohammed J. Zaki, Jeffrey Xu Yu, Balaraman Ravindran and Vikram Pudi (eds.). Springer Berlin Heidelberg (2010), 391–402.  

Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent dirichlet allocation. In: NIPS, pp. 601–608. MIT Press (2001)

Resende G., Melo P., Reis J. C. S., Vasconcelos M., Almeida J. M., Benevenuto F. Analyzing Textual (Mis)Information Shared in WhatsApp Groups. In Proceedings of the 10th ACM Conference on Web Science (WebSci '19). Association for Computing Machinery, New York, NY, USA (2019), 225–234.  

Roberts M., Stewart B., Tingley D., and Airoldi E. The Structural Topic Model and Applied Social Science (2013), Prepared for the NIPS 2013 Workshop on Topic Models: Computation, Application, and Evaluation.  




---  

## Packages & Libraries  


Required packages & libraries
```{r, warning = FALSE, message = FALSE}
packages <- c("textstem","tokenizers","tidytext","dplyr","stringr","corpus","tidyverse","stopwords","SnowballC","tidyr","topicmodels","ldatuning","wordcloud","stm","Rtsne","ggrepel")

install.packages(setdiff(packages, rownames(installed.packages())))  
```


```{r, warning = FALSE, message = FALSE}
# Import Libraries
library(textstem)
library(tokenizers)
library(tidytext)
library(dplyr)
library(stringr)
library(corpus)
library(tidyverse) 
library(stopwords)
library(SnowballC)
library(tidyr)
library(topicmodels)
library(ldatuning)
library(wordcloud)
library(stm)
library(plotly) 
library(Rtsne)
library( ggrepel)
```


```{r, include = FALSE}
 
putZeros<- function(OB) {
  OB<- OB %>% mutate_at(vars(-group_cols()),~replace(.,is.na(.),0))
  return(OB)
}
mytSNE<- function(thematrix){
  perplex<- round(sqrt(nrow(thematrix)))
  res<- Rtsne::Rtsne(thematrix, dims= 2, perplexity= perplex, check_duplicates = FALSE)
  resdf<- data.frame(x= res$Y[,1], y= res$Y[,2])
  resdf$x<- resdf$x + rnorm(nrow(resdf),0, 0.2)  # Add some noise 
  resdf$y<- resdf$y + rnorm(nrow(resdf),0, 0.2)
  return(resdf)
}
#https://rpubs.com/CelMcC/645438 
#2020
#Cel McCracken
```


## Reading in the dataset (csv)

Reading in the dataset from github. The dataset can also be found on kaggle:

https://www.kaggle.com/datasets/stevenpeutz/misinformation-fake-news-text-dataset-79k

```{r}
df = read.csv('https://raw.githubusercontent.com/StevenPeutz/Misinformation-textAnalysis/main/data/EXTRA_RussianPropagandaSubset.csv', na.strings=c("","","NA"), header=FALSE)
names(df) <- c('id','text')

```


Overview of the dataframe and detecting missing values
```{r}
sum(is.na(df$text)) 
dim(df)
str(df)
```

Exclude missing values and removing duplicate texts
```{r}
df <- na.omit(df)
df1 <- df %>% distinct(text, .keep_all = TRUE)
```

## Sampling
Taking a sample (SRS) to work with for now, to decrease processing time.
```{r}
set.seed(10)
df2 <- sample_n(df1, 2000) 
 
```


```{r}
df2  %>% 
 
  filter(str_detect(text, "^russia")) %>% 
 
  dplyr::select(text)
 
```

## Data preprocessing
```{r}
df2$clean_text <- df2$text %>% 
  str_remove_all("[^a-zA-Z\\s]")  %>% 
  # Remove URLs
  str_remove_all(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)") %>%
  # Replace "&" character reference with "and"
  str_replace_all("&amp;", "and") %>%
  # Remove nbsp 
  str_replace_all("nbsp", " ") %>%
  # Replace USSR to Soviet
  str_replace_all("USSR", "Soviet") %>%
  # Replace US to USA 
  str_replace_all("US", "USA") %>%
  # Replace US to USA 
  #str_replace_all("America", "USA") %>%
  # Replace European Union to EU
  str_replace_all("European Union", "EU") %>%
  # Replace European Union to EU
  str_replace_all("European Court", "EU") %>%
  # Remove puntucation, using a standard character class
  str_remove_all("[[:punct:]]") %>%
  # Replace any newline characters with a space
  str_replace_all("\\\n", " ") %>%
  # Remove any trailing whitespace around the text
  str_trim("both") %>%
  # Make everything lowercase
  str_to_lower()
 
```


Let's have a look at the lengths of the documents. We have to do this in order to consider whether this appropriate for LDA topic modelling.

We know from established studies that LDA is an appropriate technique in the context of tweets. This is interesting because:

A) we know LDA can work with fewer than 280 character texts as 280 is the max character length of tweets (ours is on average 241).

B) we can look at which hyper parameters are successfully used in LDA specifically when working on tweet data as this might be helpful for us as well given the lengths.

In the end we will validate afterwards by manually (sample based) checking correctness of our 'clusters'. Here we can always change our documents to the full texts instead of these document containing the summary / main claim made in the document. However, given the aim of this project, it is more interesting to use the shorter texts.


```{r}
df2  %>% 
  mutate(clean_text_1 = textclean::replace_contraction(.$clean_text),
         len_clean_text = nchar(clean_text_1),
         len_25 = ifelse(len_clean_text > 25, "More than 25", "Less than 25"),
         word = str_split(clean_text_1, "\\W+")
         ) %>% 
  summarise(mean = mean(len_clean_text))
 
```


Since our dataframe is too large to eyeball over, let's search for any row that is shorter than 25 characters. This will help us spot if there is a mistake (like a single word or tag as a whole text), but also in general help us spot how many rows might be of little value to LDA due to the shortness.

The chosen length and filtering these out or leaving them in can be experimented with later on.

 
```{r}
df2  %>% 
  mutate(clean_text_1 = textclean::replace_contraction(.$clean_text),
         len_clean_text = nchar(clean_text_1),
         len_25 = ifelse(len_clean_text > 25, "More than 25", "Less than 25"),
         word = str_split(clean_text_1, "\\W+")
         ) %>% 
  group_by(len_25) %>% 
  summarise(quantity = n())
 
```


## Stemming
We need to compare stemming methods as this will be crucial for given that the nature of these texts mean there will be a lot of demonyms used, which will can be important in determining topics. Yet we do want for example 'russia' and 'russian' to become the same 'stemmed' word e.g. 'Russi' or'Russ',  and the same for 'ukraine' and 'ukranian', while being careful about what this does to e.g.  'eu' and 'europe'. (In python this can be accomplished with the Lancaster stemmer).


Comparing Stemmers:  
- stem_words (1)  
- lemmatize_words (2, a more intensive but more linguistically correct method called lemmatization)  
- stem_snowball (3)    
- wordStem (4)    

```{r}
df2  %>% 
  mutate(clean_text_1 = textclean::replace_contraction(.$clean_text),
         len_clean_text = nchar(clean_text_1),
         word = str_split(clean_text_1, "\\W+")) %>% 
  unnest(word)  %>% 
  anti_join(get_stopwords(source = "stopwords-iso")) %>% 
  filter(str_detect(word, "^russia")) %>% 
  mutate(stem = stem_words(word),
         stem1 = lemmatize_words(word),
         stem2 = stem_snowball(word, algorithm = "en"),
         stem3 =  wordStem(word)) %>% 
  dplyr::select(word, stem, stem1, stem2, stem3)
 
```

#
#
##
#
```{r}
df2  %>% 
  mutate(clean_text_1 = textclean::replace_contraction(.$clean_text),
         len_clean_text = nchar(clean_text_1),
         word = str_split(clean_text_1, "\\W+")) %>% 
  unnest(word)  %>% 
  anti_join(get_stopwords(source = "stopwords-iso")) %>% 
  filter(str_detect(word, "^us")) %>% 
  mutate(stem = stem_words(word),
         stem1 = lemmatize_words(word),
         stem2 = stem_snowball(word, algorithm = "en"),
         stem3 =  wordStem(word)) %>% 
  dplyr::select(word, stem, stem1, stem2, stem3)
 
```

```{r}
df3 <- df2  %>% 
  mutate(clean_text_1 = textclean::replace_contraction(.$clean_text),
         len_clean_text = nchar(clean_text_1),
         word = str_split(clean_text_1, "\\W+")) %>% 
  unnest(word)  %>% 
  anti_join(get_stopwords(source = "stopwords-iso")) %>% 
  mutate(stem = stem_words(word) ) 
```


Let's see our counts without having solved the denonym problem

```{r}
df3 %>% 
  group_by(stem) %>% 
  summarise(count = n())  %>% 
  arrange(desc(count)) %>% 
  top_n(10) %>% 
  ggplot(aes(x = reorder(stem, -count), y = count)) + 
  geom_bar(stat='identity')
 
```

We can see that denonym use is enormous, we will have to fix this.  
We can use 'replace_all' on the most common denonyms identified above, to manually 'stem' these.
```{r}
df3$stem <- df3$stem %>%   
  str_replace_all("ukrainian", "ukrain") %>%
  str_replace_all("russian", "russia") %>%
  str_replace_all("russia", "99" ) %>%
  str_replace_all("european", "europ") %>% #since 'europe' already is stemmed to 'europ', but 'european' was not. 'eu' remains a separate term due its 'union' context. (rethough this, better as just eu, as european union is seperated and now is added to europe count, while eu is still separate as eu..)
  str_replace_all("syrian", "syria") %>%
  str_replace_all("belarusian", "belaru") %>%
  str_replace_all("polish", "poland") %>%
  str_replace_all("armenian", "armenia") %>%
  str_replace_all("german", "germany") %>%
  str_replace_all("germanyi", "germany") %>%
  str_replace_all("georgian", "georgia") %>%
  str_replace_all("western", "west") %>%
  str_replace_all("american", "america") %>%
  str_replace_all("usa", "america") %>%
  str_replace_all("swedish", "sweden") %>%
  str_replace_all("eastern", "east") %>%
  str_replace_all("britain", "british") %>% 
  str_replace_all("uk ", "british") %>%
  str_trim("both") 
  #str_replace_all("uk", "british") # this one goes wrong for some reason
df3 <- df3[!df3$stem=="99",]
```

Let's count words again (regardless of which text) and visualize in order to get a rough view of the content an its topics as a whole, now that we have fixed our denonym problem.
```{r}
 df3 %>% 
  group_by(stem) %>% 
  summarise(count = n())  %>% 
  arrange(desc(count)) %>% 
  top_n(10) %>% 
  ggplot(aes(x = reorder(stem, -count), y = count)) + 
  geom_bar(stat='identity')
 
```

This is already very useful in deciding our next steps.

We can see the following;

With regards to the actual content, we here see our first hint -from articles published between Jan 25th 2015 and Jan 2nd 2020!- that Ukrain was somewhat of an obsession in pro-Kremlin media.. //could this in hindsight have had some sort of predictive value when it comes to the invasion we are currently witnessing?   
(remember, the invasion we are seeing now was originally expected to be nothing more than bluff or fear mongenering by many)

```{r}
word_count <- df3 %>% 
  count(id, stem, sort = TRUE)  %>% 
  bind_tf_idf(stem, id,   n)
 
```

## Topic Modelling Prep

Creating a Document Term Matrix
```{r}
text_dtm <- word_count %>%
#  filter(tf_idf > 0.00006) %>% 
#  filter(n>1) %>% 
  cast_dtm(id, stem, n) 
 
```

Finding the optimal number of topics  

For this we use the lda tuning package which has four performance tuning metrics, that are calculated for each iteration with a different k topics. As this is very computation intensive we are using two metrics instead of the recommend four for now. (More details on lda tuning and the four metrics can be found in Arun et el., 2010)  
```{r}
optimal_k_topics <- ldatuning::FindTopicsNumber(
  text_dtm,
  topics = seq(from = 3, to = 10, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 10),
  verbose = TRUE
)
# The best number of topics shows low values for CaoJuan2009 and high values for Griffith2004
FindTopicsNumber_plot(optimal_k_topics)
 
```
Here the CaoJuan2009 metric suggest a maximum topics of 5. The Deveaud2014 metric is more linear here and has a less clear cutoff, but we can see a sudden steepening of the curve after 5 topics. Combining the two, we will set our STM and LDA models to 5 topics. 





## Topic modelling > STM  
Document Term Matrix for topic modelling and estimation of the STM topic model algorithm.  
The Structural Topic Model (STM) algorithm is in essence a correlated topic model algorithm that is expended to be able to use metadata (such as publication, author, date, shares etc as added features), and is able to provide insight into the weight these metadata features play in the model (Roberts, 2013). 
```{r, warning = FALSE}
text_dfm <- df3  %>% 
  count(id, stem, sort = TRUE) %>% 
  cast_dfm(id, stem, n)
 
```
```{r, include = FALSE}
 
stm_model <- stm(text_dfm, K = 4, init.type = "Spectral")
```

Top 10 words per topic
```{r, warning = FALSE}
stm_beta <- tidy(stm_model)
stm_beta %>% 
  group_by(topic) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(term = reorder(term, beta)) %>% 
  ggplot(aes(term, beta, fill = topic))+
  geom_col(show.legend = FALSE)+
  facet_wrap(~topic, scales = "free")+
  coord_flip()
```

Topic per each text
```{r}
stm_gamma <- tidy(stm_model, matrix = "gamma",
                  document_names = rownames(text_dfm)) 

stm_topics <- stm_gamma %>% 
  spread( key = topic, value = gamma) %>% 
  dplyr::select(`1` ,  `2`  , `3`  , `4`)

stm_topic_doc <-stm_gamma %>% 
  spread( key = topic, value = gamma) %>% 
  rowwise() %>% 
  mutate(dominant = max(`1`,`2`,`3`,`4`) 
  ) %>% 
  ungroup() %>% 
  mutate(topic_dominant = max.col(stm_topics , "first")) %>% 
  rename(cluster_1 = `1` ,
         cluster_2 = `2`,
         cluster_3 = `3`,
         cluster_4 = `4`)

```



## Primary Validation of STM using tSNE  
The tSNE method (t-distributed Stochastic Neighbor Embedding) is a dimensionality reduction method, used mainly for visualization of data in 2D and 3D maps. This method can find non-linear connections in the data, in this manner we can get a visualization of the topics and their distances as cluster reduced to a 2-dimensional level. 
By itself the TSNE visualization remains inconclusive as to validate a certain clustering or topic modelling, but it can be very insightful when used to compare different models on the same data.
```{r}
stm_gamma %>% 
  ggplot(aes(gamma, fill = as.factor(topic)))+
  geom_histogram(show.legend = FALSE)+
  facet_wrap(~topic, ncol = 2)
 
```


```{r}
dt <- stm_model %>% 
  tidy(matrix = "gamma")  %>% 
  spread(topic,gamma) %>%
  putZeros() %>%
  select(-document)  
 

resdf <- mytSNE(dt) %>%
  mutate(cluster= stm_topic_doc$topic_dominant   , 
         text= str_wrap(df2$clean_text, 25))
 
resdf %>% 
  ggplot(aes(x = x, y =y, color = as.factor(cluster)))+
  geom_point() 
 
```

## Topic modelling > LDA  
We use Latent Dirichlet topic modelling (LDA) in order to create our mixed membership model. We use LDA because it is an algorithm that uses a three level hierarchical Bayesian model in which each item of a collection is modeled as a finite mixture over an underlying set of topics (Blei et al., 2001).  
This in practical terms allows us to take into account that text might contain e.g. two topics instead of simply one.
```{r}
lda_model <- LDA(text_dtm, k = 4, control = list(seed = 10)) 
 
```


Topic per each text
```{r}
lda_result <- posterior(lda_model)
 
 
```

Top 10 words per topic
```{r}
lda_beta <- tidy(lda_model, matrix = "beta") 

lda_beta %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>% 
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 4) +
  coord_flip() 
```

## Primary Validation of LDA using tSNE  
Looking at the tSNE visualization -in comparison to that of the STM model- shows a far greater distance between the topics on this 2-dimensional pane, yet with equally or even smaller distances within each topic. This is suggestive of an accurate topic model.   
(As described before, he tSNE method is a dimensionality reduction method, used mainly for visualization of data in 2D and 3D maps. This method can find non-linear connections in the data, in this manner we can get a visualization of the topics and their distances as cluster reduced to a 2-dimensional level.)

```{r}
lda_gamma <- tidy(lda_model, matrix = "gamma") 
 
lda_gamma %>% 
  ggplot(aes(gamma, fill = as.factor(topic)))+
  geom_histogram(show.legend = FALSE)+
  facet_wrap(~topic, ncol = 2)
```

```{r}
lda_topics <- lda_gamma %>% 
  spread( key = topic, value = gamma) %>% 
  dplyr::select(`1`,`2`, `3`, `4`)

lda_topic_doc <-lda_gamma %>% 
  spread( key = topic, value = gamma) %>% 
  rowwise() %>% 
  mutate(dominant = max(`1`,`2`,`3`,`4`) 
  ) %>% 
  ungroup() %>% 
  mutate(topic_dominant = max.col(lda_topics , "first")) %>% 
  rename(cluster_1 = `1` ,
         cluster_2 = `2`,
         cluster_3 = `3`,
         cluster_4 = `4`)



lda_dt <- lda_model %>% 
  tidy(matrix = "gamma")  %>% 
  spread(topic,gamma) %>%
  putZeros() %>%
  select(-document)  
 

lda_resdf <- mytSNE(lda_dt) %>%
  mutate(cluster= lda_topic_doc$topic_dominant )
 
lda_resdf %>% 
  ggplot(aes(x = x, y =y, color = as.factor(cluster)))+
  geom_point() 
```

## Topic Interpretation

Wordcloud of top 20 words, Topic 1
```{r, warning = FALSE}
 
top5termsPerTopic <- terms(lda_model, 5)

topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")

# visualize topics as word cloud
topicToViz <- 1 # change for your own topic of interest
 # select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(lda_result$terms[topicToViz,], 
                   decreasing=TRUE)[1:20]
words <- names(top40terms)
# extract the probabilites of each of the 20 terms
probabilities <- sort(lda_result$terms[topicToViz,], 
                      decreasing=TRUE)[1:20]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
set.seed(10)
wordcloud(words, probabilities, random.order = FALSE, 
          color = mycolors, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"), min.freq = 1,
          max.words=200 )
 
 
```
 
Wordcloud of top 20 words, Topic 2
```{r, warning = FALSE}
# visualize topics as word cloud
topicToViz <- 2 # change for your own topic of interest
 # select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(lda_result$terms[topicToViz,], 
                   decreasing=TRUE)[1:20]
words <- names(top40terms)
# extract the probabilites of each of the 20 terms
probabilities <- sort(lda_result$terms[topicToViz,], 
                      decreasing=TRUE)[1:20]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
set.seed(10)
wordcloud(words, probabilities, random.order = FALSE, 
          color = mycolors, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"), min.freq = 1,
          max.words=200 )
 
```

Wordcloud of top 20 words, Topic 3
```{r, warning = FALSE}
 # visualize topics as word cloud
topicToViz <- 3 # change for your own topic of interest
 # select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(lda_result$terms[topicToViz,], 
                   decreasing=TRUE)[1:20]
words <- names(top40terms)
# extract the probabilites of each of the 20 terms
probabilities <- sort(lda_result$terms[topicToViz,], 
                      decreasing=TRUE)[1:20]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
set.seed(10)
wordcloud(words, probabilities, random.order = FALSE, 
          color = mycolors, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"), min.freq = 1,
          max.words=200 )
 
```

Wordcloud of top 20 words, Topic 4
```{r, warning = FALSE}
# visualize topics as word cloud
topicToViz <- 4 # change for your own topic of interest
 # select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(lda_result$terms[topicToViz,], 
                   decreasing=TRUE)[1:20]
words <- names(top40terms)
# extract the probabilites of each of the 20 terms
probabilities <- sort(lda_result$terms[topicToViz,], 
                      decreasing=TRUE)[1:20]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
set.seed(10)
wordcloud(words, probabilities, random.order = FALSE, 
          color = mycolors, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"), min.freq = 1,
          max.words=200 )
  
 
```


#Manual validation
For a second form of validation we will use manual 'expert' labelling anc compare results with the results predicted by our model. 
 

With our LDA topic model based on the 2000 text sample we have the following 4 topics:  
1 'Chemical attack Syria' (also containing: america(n), syria(n), atack, chemic(al), czech(ia), trump)
2 'War at bigger scope' (keywords: nato, sanctions, europe, military, war, poland, soviet, georgia)
3 'Ukraine war' (keywords: ukraine, porochenko, donbas, crimea, kyiv)
4 'Picking sides' (keywords: america, west(ern), belarus, germany, eu, europe, countries, nations)  
 

Sampling
```{r}
set.seed(10)
df_manual <- sample_n(df2, 30)
df_manual
```
My random sample has given me the text with the following ID's; 1607	4158	298	6395	3920	6319	3682	2842	6166	7263.  
Reading through the raw texts (without stemming or other changes, we assign the them to topics.  
1607 -> topic: 3 | 4  
4158 -> topic: 1 | 4  
298 -> topic: 2 | 5   
6395 -> topic: 5 | 1  
3920 -> topic: undecided : undecided (2?)  
6319	-> topic: 4 (partly topic 2) | undecided(5?)  
3682	-> topic: undecided (2 and 4) | undecided  
2842	-> topic: 2 (partly topic 4) | 4  
6166	-> topic: 2 (partly topic 1) | 4  
7263 -> topic: 3 (partly topic 4) | 2   
 

```{r}
df_manual %>% 
  mutate(document = as.character(id)) %>% 
  left_join(lda_topic_doc) %>% 
  dplyr::select(document, text, topic_dominant)
 
```



















