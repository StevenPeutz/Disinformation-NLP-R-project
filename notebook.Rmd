---
title: "Pro-Kremlin Misinformation"
output:
  html_document:
    df_print: paged
---
 

## About the data:

Disinformation and propaganda cases collected by the EUvsDisinfo project. A project started in 2015 that identifies and fact checks disinformation cases originating from pro-Kremlin media that are spread across the EU.


## Why topic modelling, why not add regular 'true' news articles and use classification (i.e. misinfo prediction?)

While this might immediately seem like a great classification task, I would suggest also considering clustering / topic modelling. Why clustering? Because by clustering we make a model that can match a newly written article to a previously debunked lie / misinformation narrative, thereby we can immediately debunk a new article without either using an algorithm as argument, or encountering a time delay with regards to waiting for confirmation of a fact checking organisation.

```{r, warning = FALSE, message = FALSE}
packages <- c("textstem","tokenizers","tidytext","dplyr","stringr","corpus","tidyverse","stopwords","SnowballC","tidyr","topicmodels","ldatuning","wordcloud","stm","Rtsne","ggrepel")

install.packages(setdiff(packages, rownames(installed.packages())))  
```


Libraries
```{r, warning = FALSE, message = FALSE}
# Import Libraries
library(textstem)
library(tokenizers)
library(tidytext)
library(dplyr)
library(stringr)
library(corpus)
library(tidyverse) 
library(stopwords)
library(SnowballC)
library(tidyr)
library(topicmodels)
library(ldatuning)
library(wordcloud)
library(stm)
library(plotly) 
library(Rtsne)
library( ggrepel)
```


```{r, include = FALSE}
 
putZeros<- function(OB) {
  OB<- OB %>% mutate_at(vars(-group_cols()),~replace(.,is.na(.),0))
  return(OB)
}
mytSNE<- function(thematrix){
  perplex<- round(sqrt(nrow(thematrix)))
  res<- Rtsne::Rtsne(thematrix, dims= 2, perplexity= perplex, check_duplicates = FALSE)
  resdf<- data.frame(x= res$Y[,1], y= res$Y[,2])
  resdf$x<- resdf$x + rnorm(nrow(resdf),0, 0.2)  # Add some noise 
  resdf$y<- resdf$y + rnorm(nrow(resdf),0, 0.2)
  return(resdf)
}
#https://rpubs.com/CelMcC/645438 
#2020
#Cel McCracken
```

## Reading in the dataset (csv)

Reading in the dataset from github. The dataset with description added by me can also be found on kaggle here:

https://www.kaggle.com/datasets/stevenpeutz/misinformation-fake-news-text-dataset-79k
```{r}
df = read.csv('https://raw.githubusercontent.com/StevenPeutz/Misinformation-textAnalysis/main/data/EXTRA_RussianPropagandaSubset.csv', na.strings=c("","","NA"), header=FALSE)
names(df) <- c('id','text')

```


Ovierview of the dataframe and detecting missing values
```{r}
sum(is.na(df$text)) 
dim(df)
str(df)
```
Exclude missing values
```{r}
df <- na.omit(df)
```
 
```{r}
df2 <- sample_n(df, 100) 
 
```

Data preprocessing
```{r}
df2$clean_text <- df2$text %>% 
  # Make everything lowercase
  str_to_lower() %>%
  str_remove_all("[^a-zA-Z\\s]")  %>% 
  # Remove URLs
  str_remove_all(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)") %>%
  # Replace "&" character reference with "and"
  str_replace_all("&amp;", "and") %>%
  # Remove puntucation, using a standard character class
  str_remove_all("[[:punct:]]") %>%
  # Replace any newline characters with a space
  str_replace_all("\\\n", " ") %>%
  # Remove any trailing whitespace around the text
  str_trim("both") 
 
```

Let's have a look at the lengths of the documents. We have to do this in order to consider whether this appropriate for LDA topic modelling.

We know that LDA is an approprate techinque in the context of tweets. This is interesting because:

A) we know LDA can work with fewer than 280 character texts as 280 is the max character length of tweets (ours is on average 257).

B) we can look at which hyperparameter are succesfully used in LDA when working on tweet data as this might be helpful for us as well.

In the end we will validate aftwards by manually (sample based) checking correctness of clustering. Here we can always change of documents to the full texts instead of these document containing the summary / main claim made in the document.
```{r}
df2  %>% 
  mutate(clean_text_1 = textclean::replace_contraction(.$clean_text),
         len_clean_text = nchar(clean_text_1),
         len_25 = ifelse(len_clean_text > 130, "More than 25", "Less than 25"),
         word = str_split(clean_text_1, "\\W+")
         ) %>% 
  summarise(mean = mean(len_clean_text))
 
```

Since our series is too large to eyeball over, let's search for any row that is shorter than 25 characters. This will help us spot if there is a mistake (like a single word or tag as a whole text), but also in general help us spot
# how many rows will be of little value to LDA due to the shortness.
 
```{r}
df2  %>% 
  mutate(clean_text_1 = textclean::replace_contraction(.$clean_text),
         len_clean_text = nchar(clean_text_1),
         len_25 = ifelse(len_clean_text > 130, "More than 25", "Less than 25"),
         word = str_split(clean_text_1, "\\W+")
         ) %>% 
  group_by(len_25) %>% 
  summarise(quantity = n())
 
```

 
Comparison Stemming
```{r}
df2  %>% 
  mutate(clean_text_1 = textclean::replace_contraction(.$clean_text),
         len_clean_text = nchar(clean_text_1),
         word = str_split(clean_text_1, "\\W+")) %>% 
  unnest(word)  %>% 
  anti_join(get_stopwords(source = "stopwords-iso")) %>% 
  filter(str_detect(word, "^russia")) %>% 
  mutate(stem = stem_words(word),
         stem1 = lemmatize_words(word),
         stem2 = stem_snowball(word, algorithm = "en"),
         stem3 =  wordStem(word)) %>% 
  dplyr::select(word, stem, stem1, stem2, stem3)
 
```


```{r}
df3 <- df2  %>% 
  mutate(clean_text_1 = textclean::replace_contraction(.$clean_text),
         len_clean_text = nchar(clean_text_1),
         word = str_split(clean_text_1, "\\W+")) %>% 
  unnest(word)  %>% 
  anti_join(get_stopwords(source = "stopwords-iso")) %>% 
  mutate(stem = stem_words(word) ) 
```



```{r}
df3 %>% 
  group_by(stem) %>% 
  summarise(count = n())  %>% 
  arrange(desc(count)) %>% 
  top_n(10) %>% 
  ggplot(aes(x = reorder(stem, -count), y = count)) + 
  geom_bar(stat='identity')
 
```

Replacing top words 
```{r}
df3$stem <- df3$stem %>%   
  str_replace_all("ukrainian", "ukrain") %>%
  str_replace_all("russian", "russia")
 
```

Let's count words (regardless of which text) and visualize to get a rough view of the content an its topics as a whole.
```{r}
 df3 %>% 
  group_by(stem) %>% 
  summarise(count = n())  %>% 
  arrange(desc(count)) %>% 
  top_n(10) %>% 
  ggplot(aes(x = reorder(stem, -count), y = count)) + 
  geom_bar(stat='identity')
 
```
This is already very useful in deciding our next steps.

We can see the following;

The need to lemmatize/stem) see for example russia and russian, west and western etc. // while being careful about what this does to 'eu' and 'europe'.
If we were to use tf-idf, we probably want to remove 'russia' (its stems) first. However, we would not want to remove Ukrain eventhough the difference is not even that large. This because where a selfreference using a country name is simply conceptually very different from using a neighbouring country's name, in terms of meaning. This means we have to be carefull when using the parameter that removes word based on cross document occurance.
With regards to the actual content, we here see our first hint -from articles published between Jan 25th 2015 and Jan 2nd 2020!- that Ukrain was somewhat of an obsession in pro-Kremlin media.. //could this in hindsight have been a predictor of the invasion?

```{r}
word_count <- df3 %>% 
  count(id, stem, sort = TRUE)  %>% 
  bind_tf_idf(stem, id,   n)
 
```

Creating a Document Term Matrix
```{r}
text_dtm <- word_count %>%
#  filter(tf_idf > 0.00006) %>% 
#  filter(n>1) %>% 
  cast_dtm(id, stem, n) 
 
```

Finding the optimal number of topics
```{r}
optimal_k_topics <- ldatuning::FindTopicsNumber(
  text_dtm,
  topics = seq(from = 3, to = 10, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 10),
  verbose = TRUE
)
# The best number of topics shows low values for CaoJuan2009 and high values for Griffith2004
FindTopicsNumber_plot(optimal_k_topics)
 
```

Document Term Matrix for topic modelling and estimation of the STM topic model algorithm
```{r, warning = FALSE}
text_dfm <- df3  %>% 
  count(id, stem, sort = TRUE) %>% 
  cast_dfm(id, stem, n)
 
```
```{r, include = FALSE}
 
stm_model <- stm(text_dfm, K = 4, init.type = "Spectral")
```

Top 10 words per topic
```{r, warning = FALSE}
stm_beta <- tidy(stm_model)
stm_beta %>% 
  group_by(topic) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(term = reorder(term, beta)) %>% 
  ggplot(aes(term, beta, fill = topic))+
  geom_col(show.legend = FALSE)+
  facet_wrap(~topic, scales = "free")+
  coord_flip()
```
Topic per each text
```{r}
stm_gamma <- tidy(stm_model, matrix = "gamma",
                  document_names = rownames(text_dfm)) 

stm_topics <- stm_gamma %>% 
  spread( key = topic, value = gamma) %>% 
  dplyr::select(`1` ,  `2`  , `3`  , `4`)

stm_topic_doc <-stm_gamma %>% 
  spread( key = topic, value = gamma) %>% 
  rowwise() %>% 
  mutate(dominant = max(`1`,`2`,`3`,`4`) 
  ) %>% 
  ungroup() %>% 
  mutate(topic_dominant = max.col(stm_topics , "first")) %>% 
  rename(cluster_1 = `1` ,
         cluster_2 = `2`,
         cluster_3 = `3`,
         cluster_4 = `4`)

```



# Validate topic modelling 
```{r}
stm_gamma %>% 
  ggplot(aes(gamma, fill = as.factor(topic)))+
  geom_histogram(show.legend = FALSE)+
  facet_wrap(~topic, ncol = 2)
 
```


```{r}
dt <- stm_model %>% 
  tidy(matrix = "gamma")  %>% 
  spread(topic,gamma) %>%
  putZeros() %>%
  select(-document)  
 

resdf <- mytSNE(dt) %>%
  mutate(cluster= stm_topic_doc$topic_dominant   , 
         text= str_wrap(df2$clean_text, 25))
 
resdf %>% 
  ggplot(aes(x = x, y =y, color = as.factor(cluster)))+
  geom_point() 
 
```

 
Latent Dirichlet Allocation (LDA)
```{r}
lda_model <- LDA(text_dtm, k = 4, control = list(seed = 10)) 
 
```


Topic per each text
```{r}
lda_result <- posterior(lda_model)
 
 
```

Top 10 words per topic
```{r}
lda_beta <- tidy(lda_model, matrix = "beta") 

lda_beta %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>% 
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 4) +
  coord_flip() 
```

# Validate results
Excellent job since our topic modelling take the texts into 1 topic. The topic modelling assures that the probability of the text to belong to a topic is very high. Very strong associated

```{r}
lda_gamma <- tidy(lda_model, matrix = "gamma") 
 
lda_gamma %>% 
  ggplot(aes(gamma, fill = as.factor(topic)))+
  geom_histogram(show.legend = FALSE)+
  facet_wrap(~topic, ncol = 2)
```

```{r}
lda_topics <- lda_gamma %>% 
  spread( key = topic, value = gamma) %>% 
  dplyr::select(`1` ,  `2`  , `3`  , `4`)

lda_topic_doc <-lda_gamma %>% 
  spread( key = topic, value = gamma) %>% 
  rowwise() %>% 
  mutate(dominant = max(`1`,`2`,`3`,`4`) 
  ) %>% 
  ungroup() %>% 
  mutate(topic_dominant = max.col(lda_topics , "first")) %>% 
  rename(cluster_1 = `1` ,
         cluster_2 = `2`,
         cluster_3 = `3`,
         cluster_4 = `4`)



lda_dt <- lda_model %>% 
  tidy(matrix = "gamma")  %>% 
  spread(topic,gamma) %>%
  putZeros() %>%
  select(-document)  
 

lda_resdf <- mytSNE(lda_dt) %>%
  mutate(cluster= lda_topic_doc$topic_dominant )
 
lda_resdf %>% 
  ggplot(aes(x = x, y =y, color = as.factor(cluster)))+
  geom_point() 
```

Wordcloud of top 20 words, Topic 1
```{r, warning = FALSE}
 
top5termsPerTopic <- terms(lda_model, 5)

topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")

# visualize topics as word cloud
topicToViz <- 1 # change for your own topic of interest
 # select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(lda_result$terms[topicToViz,], 
                   decreasing=TRUE)[1:20]
words <- names(top40terms)
# extract the probabilites of each of the 20 terms
probabilities <- sort(lda_result$terms[topicToViz,], 
                      decreasing=TRUE)[1:20]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
set.seed(10)
wordcloud(words, probabilities, random.order = FALSE, 
          color = mycolors, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"), min.freq = 1,
          max.words=200 )
 
 
```
 
Wordcloud of top 20 words, Topic 2
```{r, warning = FALSE}
# visualize topics as word cloud
topicToViz <- 2 # change for your own topic of interest
 # select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(lda_result$terms[topicToViz,], 
                   decreasing=TRUE)[1:20]
words <- names(top40terms)
# extract the probabilites of each of the 20 terms
probabilities <- sort(lda_result$terms[topicToViz,], 
                      decreasing=TRUE)[1:20]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
set.seed(10)
wordcloud(words, probabilities, random.order = FALSE, 
          color = mycolors, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"), min.freq = 1,
          max.words=200 )
 
```

Wordcloud of top 20 words, Topic 3
```{r, warning = FALSE}
 # visualize topics as word cloud
topicToViz <- 3 # change for your own topic of interest
 # select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(lda_result$terms[topicToViz,], 
                   decreasing=TRUE)[1:20]
words <- names(top40terms)
# extract the probabilites of each of the 20 terms
probabilities <- sort(lda_result$terms[topicToViz,], 
                      decreasing=TRUE)[1:20]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
set.seed(10)
wordcloud(words, probabilities, random.order = FALSE, 
          color = mycolors, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"), min.freq = 1,
          max.words=200 )
 
```

Wordcloud of top 20 words, Topic 4
```{r, warning = FALSE}
# visualize topics as word cloud
topicToViz <- 4 # change for your own topic of interest
 # select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(lda_result$terms[topicToViz,], 
                   decreasing=TRUE)[1:20]
words <- names(top40terms)
# extract the probabilites of each of the 20 terms
probabilities <- sort(lda_result$terms[topicToViz,], 
                      decreasing=TRUE)[1:20]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
set.seed(10)
wordcloud(words, probabilities, random.order = FALSE, 
          color = mycolors, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"), min.freq = 1,
          max.words=200 )
  
 
```

```{r}
 
 
```

```{r}
 
 
```

```{r}
 
 
```

```{r}
 
 
```



















