---
title: "Pro-Kremlin Misinformation"
output:
  html_document:
    df_print: paged
---
 
## Introduction

Disinformation, defined as the subset of 'misinformation' where there is intent to mislead, has seen a an astronomical rise in both its success in terms of spread and impact as well in the effort to combat it. Though known state-backed disinformation campaigns date back to at least the cold war era, they have perhaps caught the eye of the public after the 2016 US presidential election (Allcot et al., 2017). Later in 2016 disinformation is claimed to have been influential in the 'Brexit' referendum, and later in 2018 a similar development was suspected during the Brazilian presidential elections. As this was unfolding an explosive growth in so called 'fact-checking' organisation and their cooperation with news agencies, social media companies (e.g. facebook/meta) and governments can be seen. Fact-checking organization, often volunteer based or financed through charity, their capacity tend to be outpaced by the sheer volume of suspected disinformation content.

One supposed solution for this is using AI to automate classification of new articles or posts based on its linguistic aspects. Though improvements are being made here, the most accurate models are strongly dependent on meta data such as publication network which are often not available when shared through social media, and have other downsides such as disproportionately high false positive rates when publication networks that have shared disinformation were to post content without disinformation.

An alternative 'intermediate' solution is offered in this project. Instead of classification we aim to discover the topics that are present in Russian disinformation. By doing this we can streamline fact-checking by establishing a basis on which already fact-checked disinformation can be matched with with newly published unchecked articles and posts. We use Latent Dirichlet topic modelling (LDA) in order to create our mixed membership model. We use LDA because it is an algorithm that uses a three level hierarchical Bayesian model in which each item of a collection is modeled as a finite mixture over an underlying set of topics (Blei et al., 2001).



## Project scope

This projects restricts itself to pro-Kremlin disinformation. This is specified as 'pro-Kremlin' as direct ties with the Russian Internet Research Agency (IRA) and official backing of the Kremlin are perhaps expected but are not verified with hard proof.
The scoped will also be limited to a preliminary test of the theoretical possibility and validity of the LDA mixed membership modelling without going into the practical application of the results.



## About the data:

The disinformation texts were collected by the EUvsDisinfo project. A project started in 2015 that identifies and fact checks disinformation cases originating from pro-Kremlin media that are spread across the EU. 
More information about this project can be found here: https://euvsdisinfo.eu/
The dataset can be found here: https://www.kaggle.com/datasets/stevenpeutz/misinformation-fake-news-text-dataset-79k



## References

H. Allcott and M. Gentzkow. 2017. Social media and fake news in the 2016 election. Journal of Economic Perspectives 31, 2 (2017), 211–36.

Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent dirichlet allocation. In: NIPS, pp. 601–608. MIT Press (2001)

G. Resende, P. Melo, J. C. S. Reis, M. Vasconcelos, J. M. Almeida, F. Benevenuto. Analyzing Textual (Mis)Information Shared in WhatsApp Groups. In Proceedings of the 10th ACM Conference on Web Science (WebSci '19). Association for Computing Machinery, New York, NY, USA (2019), 225–234.


---


Required packages
```{r, warning = FALSE, message = FALSE}
packages <- c("textstem","tokenizers","tidytext","dplyr","stringr","corpus","tidyverse","stopwords","SnowballC","tidyr","topicmodels","ldatuning","wordcloud","stm","Rtsne","ggrepel")

install.packages(setdiff(packages, rownames(installed.packages())))  
```


Required libraries
```{r, warning = FALSE, message = FALSE}
# Import Libraries
library(textstem)
library(tokenizers)
library(tidytext)
library(dplyr)
library(stringr)
library(corpus)
library(tidyverse) 
library(stopwords)
library(SnowballC)
library(tidyr)
library(topicmodels)
library(ldatuning)
library(wordcloud)
library(stm)
library(plotly) 
library(Rtsne)
library( ggrepel)
```


```{r, include = FALSE}
 
putZeros<- function(OB) {
  OB<- OB %>% mutate_at(vars(-group_cols()),~replace(.,is.na(.),0))
  return(OB)
}
mytSNE<- function(thematrix){
  perplex<- round(sqrt(nrow(thematrix)))
  res<- Rtsne::Rtsne(thematrix, dims= 2, perplexity= perplex, check_duplicates = FALSE)
  resdf<- data.frame(x= res$Y[,1], y= res$Y[,2])
  resdf$x<- resdf$x + rnorm(nrow(resdf),0, 0.2)  # Add some noise 
  resdf$y<- resdf$y + rnorm(nrow(resdf),0, 0.2)
  return(resdf)
}
#https://rpubs.com/CelMcC/645438 
#2020
#Cel McCracken
```


## Reading in the dataset (csv)

Reading in the dataset from github. The dataset can also be found on kaggle:

https://www.kaggle.com/datasets/stevenpeutz/misinformation-fake-news-text-dataset-79k

```{r}
df = read.csv('https://raw.githubusercontent.com/StevenPeutz/Misinformation-textAnalysis/main/data/EXTRA_RussianPropagandaSubset.csv', na.strings=c("","","NA"), header=FALSE)
names(df) <- c('id','text')

```


Overview of the dataframe and detecting missing values
```{r}
sum(is.na(df$text)) 
dim(df)
str(df)
```

Exclude missing values
```{r}
df <- na.omit(df)
```

### Sampling
Taking a sample (SRS) to work with for now, to decrease processing time.
```{r}
df2 <- sample_n(df, 100) 
 
```

### Data preprocessing
```{r}
df2$clean_text <- df2$text %>% 
  # Make everything lowercase
  str_to_lower() %>%
  str_remove_all("[^a-zA-Z\\s]")  %>% 
  # Remove URLs
  str_remove_all(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)") %>%
  # Replace "&" character reference with "and"
  str_replace_all("&amp;", "and") %>%
  # Remove puntucation, using a standard character class
  str_remove_all("[[:punct:]]") %>%
  # Replace any newline characters with a space
  str_replace_all("\\\n", " ") %>%
  # Remove any trailing whitespace around the text
  str_trim("both") 
 
```


Let's have a look at the lengths of the documents. We have to do this in order to consider whether this appropriate for LDA topic modelling.

We know from established studies that LDA is an appropriate technique in the context of tweets. This is interesting because:

A) we know LDA can work with fewer than 280 character texts as 280 is the max character length of tweets (ours is on average 257).

B) we can look at which hyper parameters are successfully used in LDA specifically when working on tweet data as this might be helpful for us as well given the lengths.

In the end we will validate afterwards by manually (sample based) checking correctness of our 'clusters'. Here we can always change our documents to the full texts instead of these document containing the summary / main claim made in the document. However, given the aim of this project, it is more interesting to use the shorter texts.


```{r}
df2  %>% 
  mutate(clean_text_1 = textclean::replace_contraction(.$clean_text),
         len_clean_text = nchar(clean_text_1),
         len_25 = ifelse(len_clean_text > 130, "More than 25", "Less than 25"),
         word = str_split(clean_text_1, "\\W+")
         ) %>% 
  summarise(mean = mean(len_clean_text))
 
```


Since our dataframe is too large to eyeball over, let's search for any row that is shorter than 25 characters. This will help us spot if there is a mistake (like a single word or tag as a whole text), but also in general help us spot

How many rows will be of little value to LDA due to the shortness?
This can be experimented with later on.

 
```{r}
df2  %>% 
  mutate(clean_text_1 = textclean::replace_contraction(.$clean_text),
         len_clean_text = nchar(clean_text_1),
         len_25 = ifelse(len_clean_text > 130, "More than 25", "Less than 25"),
         word = str_split(clean_text_1, "\\W+")
         ) %>% 
  group_by(len_25) %>% 
  summarise(quantity = n())
 
```


#### Stemming
We need to compare stemming methods as this will be crucial for given that the nature of these texts mean there will be a lot of demonyms used, which will can be important in determining topics. Yet we do want for example 'russia' and 'russian' to become the same 'stemmed' word e.g. 'Russi' or'Russ',  and the same for 'ukraine' and 'ukranian', while being careful about what this does to e.g.  'eu' and 'europe'. (In python this can be accomplished with the Lancaster stemmer).


Comparing Stemmers:
- stem_words
- lemmatize_words (a more intensive but more linguistically correct method called lemmatization)
- stem_snowball
- wordStem

```{r}
df2  %>% 
  mutate(clean_text_1 = textclean::replace_contraction(.$clean_text),
         len_clean_text = nchar(clean_text_1),
         word = str_split(clean_text_1, "\\W+")) %>% 
  unnest(word)  %>% 
  anti_join(get_stopwords(source = "stopwords-iso")) %>% 
  filter(str_detect(word, "^russia")) %>% 
  mutate(stem = stem_words(word),
         stem1 = lemmatize_words(word),
         stem2 = stem_snowball(word, algorithm = "en"),
         stem3 =  wordStem(word)) %>% 
  dplyr::select(word, stem, stem1, stem2, stem3)
 
```


```{r}
df3 <- df2  %>% 
  mutate(clean_text_1 = textclean::replace_contraction(.$clean_text),
         len_clean_text = nchar(clean_text_1),
         word = str_split(clean_text_1, "\\W+")) %>% 
  unnest(word)  %>% 
  anti_join(get_stopwords(source = "stopwords-iso")) %>% 
  mutate(stem = stem_words(word) ) 
```


Let's see our counts without having solves the denonym problem

```{r}
df3 %>% 
  group_by(stem) %>% 
  summarise(count = n())  %>% 
  arrange(desc(count)) %>% 
  top_n(10) %>% 
  ggplot(aes(x = reorder(stem, -count), y = count)) + 
  geom_bar(stat='identity')
 
```

Let's use 'replace_all' on the most common denonyms identified above, to manually 'stem' these.
```{r}
df3$stem <- df3$stem %>%   
  str_replace_all("ukrainian", "ukrain") %>%
  str_replace_all("russian", "russia")
 
```

Let's count words (regardless of which text) and visualize in order to get a rough view of the content an its topics as a whole.
```{r}
 df3 %>% 
  group_by(stem) %>% 
  summarise(count = n())  %>% 
  arrange(desc(count)) %>% 
  top_n(10) %>% 
  ggplot(aes(x = reorder(stem, -count), y = count)) + 
  geom_bar(stat='identity')
 
```

This is already very useful in deciding our next steps.

We can see the following;

With regards to the actual content, we here see our first hint -from articles published between Jan 25th 2015 and Jan 2nd 2020!- that Ukrain was somewhat of an obsession in pro-Kremlin media.. //could this in hindsight have been a predictor of the invasion?

```{r}
word_count <- df3 %>% 
  count(id, stem, sort = TRUE)  %>% 
  bind_tf_idf(stem, id,   n)
 
```

Creating a Document Term Matrix
```{r}
text_dtm <- word_count %>%
#  filter(tf_idf > 0.00006) %>% 
#  filter(n>1) %>% 
  cast_dtm(id, stem, n) 
 
```

Finding the optimal number of topics
```{r}
optimal_k_topics <- ldatuning::FindTopicsNumber(
  text_dtm,
  topics = seq(from = 3, to = 10, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 10),
  verbose = TRUE
)
# The best number of topics shows low values for CaoJuan2009 and high values for Griffith2004
FindTopicsNumber_plot(optimal_k_topics)
 
```

Document Term Matrix for topic modelling and estimation of the STM topic model algorithm
```{r, warning = FALSE}
text_dfm <- df3  %>% 
  count(id, stem, sort = TRUE) %>% 
  cast_dfm(id, stem, n)
 
```
```{r, include = FALSE}
 
stm_model <- stm(text_dfm, K = 4, init.type = "Spectral")
```

Top 10 words per topic
```{r, warning = FALSE}
stm_beta <- tidy(stm_model)
stm_beta %>% 
  group_by(topic) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(term = reorder(term, beta)) %>% 
  ggplot(aes(term, beta, fill = topic))+
  geom_col(show.legend = FALSE)+
  facet_wrap(~topic, scales = "free")+
  coord_flip()
```
Topic per each text
```{r}
stm_gamma <- tidy(stm_model, matrix = "gamma",
                  document_names = rownames(text_dfm)) 

stm_topics <- stm_gamma %>% 
  spread( key = topic, value = gamma) %>% 
  dplyr::select(`1` ,  `2`  , `3`  , `4`)

stm_topic_doc <-stm_gamma %>% 
  spread( key = topic, value = gamma) %>% 
  rowwise() %>% 
  mutate(dominant = max(`1`,`2`,`3`,`4`) 
  ) %>% 
  ungroup() %>% 
  mutate(topic_dominant = max.col(stm_topics , "first")) %>% 
  rename(cluster_1 = `1` ,
         cluster_2 = `2`,
         cluster_3 = `3`,
         cluster_4 = `4`)

```



# Validate topic modelling 
```{r}
stm_gamma %>% 
  ggplot(aes(gamma, fill = as.factor(topic)))+
  geom_histogram(show.legend = FALSE)+
  facet_wrap(~topic, ncol = 2)
 
```


```{r}
dt <- stm_model %>% 
  tidy(matrix = "gamma")  %>% 
  spread(topic,gamma) %>%
  putZeros() %>%
  select(-document)  
 

resdf <- mytSNE(dt) %>%
  mutate(cluster= stm_topic_doc$topic_dominant   , 
         text= str_wrap(df2$clean_text, 25))
 
resdf %>% 
  ggplot(aes(x = x, y =y, color = as.factor(cluster)))+
  geom_point() 
 
```

 
Latent Dirichlet Allocation (LDA)
```{r}
lda_model <- LDA(text_dtm, k = 4, control = list(seed = 10)) 
 
```


Topic per each text
```{r}
lda_result <- posterior(lda_model)
 
 
```

Top 10 words per topic
```{r}
lda_beta <- tidy(lda_model, matrix = "beta") 

lda_beta %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>% 
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 4) +
  coord_flip() 
```

# Validate results
Excellent job since our topic modelling take the texts into 1 topic. The topic modelling assures that the probability of the text to belong to a topic is very high. Very strong associated

```{r}
lda_gamma <- tidy(lda_model, matrix = "gamma") 
 
lda_gamma %>% 
  ggplot(aes(gamma, fill = as.factor(topic)))+
  geom_histogram(show.legend = FALSE)+
  facet_wrap(~topic, ncol = 2)
```

```{r}
lda_topics <- lda_gamma %>% 
  spread( key = topic, value = gamma) %>% 
  dplyr::select(`1` ,  `2`  , `3`  , `4`)

lda_topic_doc <-lda_gamma %>% 
  spread( key = topic, value = gamma) %>% 
  rowwise() %>% 
  mutate(dominant = max(`1`,`2`,`3`,`4`) 
  ) %>% 
  ungroup() %>% 
  mutate(topic_dominant = max.col(lda_topics , "first")) %>% 
  rename(cluster_1 = `1` ,
         cluster_2 = `2`,
         cluster_3 = `3`,
         cluster_4 = `4`)



lda_dt <- lda_model %>% 
  tidy(matrix = "gamma")  %>% 
  spread(topic,gamma) %>%
  putZeros() %>%
  select(-document)  
 

lda_resdf <- mytSNE(lda_dt) %>%
  mutate(cluster= lda_topic_doc$topic_dominant )
 
lda_resdf %>% 
  ggplot(aes(x = x, y =y, color = as.factor(cluster)))+
  geom_point() 
```

Wordcloud of top 20 words, Topic 1
```{r, warning = FALSE}
 
top5termsPerTopic <- terms(lda_model, 5)

topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")

# visualize topics as word cloud
topicToViz <- 1 # change for your own topic of interest
 # select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(lda_result$terms[topicToViz,], 
                   decreasing=TRUE)[1:20]
words <- names(top40terms)
# extract the probabilites of each of the 20 terms
probabilities <- sort(lda_result$terms[topicToViz,], 
                      decreasing=TRUE)[1:20]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
set.seed(10)
wordcloud(words, probabilities, random.order = FALSE, 
          color = mycolors, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"), min.freq = 1,
          max.words=200 )
 
 
```
 
Wordcloud of top 20 words, Topic 2
```{r, warning = FALSE}
# visualize topics as word cloud
topicToViz <- 2 # change for your own topic of interest
 # select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(lda_result$terms[topicToViz,], 
                   decreasing=TRUE)[1:20]
words <- names(top40terms)
# extract the probabilites of each of the 20 terms
probabilities <- sort(lda_result$terms[topicToViz,], 
                      decreasing=TRUE)[1:20]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
set.seed(10)
wordcloud(words, probabilities, random.order = FALSE, 
          color = mycolors, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"), min.freq = 1,
          max.words=200 )
 
```

Wordcloud of top 20 words, Topic 3
```{r, warning = FALSE}
 # visualize topics as word cloud
topicToViz <- 3 # change for your own topic of interest
 # select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(lda_result$terms[topicToViz,], 
                   decreasing=TRUE)[1:20]
words <- names(top40terms)
# extract the probabilites of each of the 20 terms
probabilities <- sort(lda_result$terms[topicToViz,], 
                      decreasing=TRUE)[1:20]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
set.seed(10)
wordcloud(words, probabilities, random.order = FALSE, 
          color = mycolors, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"), min.freq = 1,
          max.words=200 )
 
```

Wordcloud of top 20 words, Topic 4
```{r, warning = FALSE}
# visualize topics as word cloud
topicToViz <- 4 # change for your own topic of interest
 # select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(lda_result$terms[topicToViz,], 
                   decreasing=TRUE)[1:20]
words <- names(top40terms)
# extract the probabilites of each of the 20 terms
probabilities <- sort(lda_result$terms[topicToViz,], 
                      decreasing=TRUE)[1:20]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
set.seed(10)
wordcloud(words, probabilities, random.order = FALSE, 
          color = mycolors, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"), min.freq = 1,
          max.words=200 )
  
 
```

```{r}
 
 
```

```{r}
 
 
```

```{r}
 
 
```

```{r}
 
 
```



















